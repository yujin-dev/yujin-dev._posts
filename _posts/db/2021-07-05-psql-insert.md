---
title: "Postgresql 빠르게 삽입하기"
category: "db"
---
지난 6월, 사내에서 새로운 데이터를 사용할 일이 생겨 DB에 데이터를 적재하였다. 데이터는 약 76833개 COMPANY 대상의 20년치 대체 데이터로 용량은 대략 300GB 정도인 것 같다.  
빅데이터라고 하기엔 용량이 애매하다만 자칫 삽입하는데 꽤 시간을 잡아먹을 것 같아 빠르게 삽입할 수 있는 방법을 알아보았다.
데이터는 기본적으로 하루 단위로 csv파일로 저장되어 있고, 파이썬으로 Postgresql에 로드할 계획이었다.

일단 Postgresql에서 데이터를 삽입하는데
- 기본적인 `INSERT TABLE_NAME(column1, column2, ..) VALUES(xx, xx, ..)` 쿼리 실행 : single row단위로 실행되기에 시간이 오래 소요됨
- `ARRAY` 형식으로 보다 빠르게 삽입할 수 있는 `UNNEST ..` : 각 칼럼별 데이터를 리스트로 추출하여 `UNNEST`로 `ARRAY` 형식으로 변환해 삽입하는데 훨씬 빨라짐
- 파일 자체를 StringByte로 출력하는 과정을 `COPY`하는 방식
- `PG BULK` 
등이 있다고 본다. 사실 더 다양한 방법이 있겠지만 일단은 위 경우들을 시도해보았다. 
아래 단계로 갈수록 데이터를 보다 빠르게 `INSERT`할 수 있는 것으로 안다. 

여기서 PYTHON으로 스크립트를 짜서 실행하려는데 마지막 PG BULK는 python sql library에서 연동되는 기능을 찾지 못해 위 3가지 방법에 대해 구현하여 실험해보고자 한다. 
Task는 <데이터 자체는 csv 파일로 존재하고 이를 DB에 적재한다> 이다.

### 1. `INSERT TABLE_NAME(column1, column2, ..) VALUES(xx, xx, ..)`


### 2. `UNNEST`
### 3. `COPY FROM`
여기서는 약간의(?) 많은 시행착오가 있었다. 일단 `copy .. from ..`은 구분자로 데이터를 SPLIT하여 읽어 삽입하는데 데이터 자체에 구분자가 포함되는 경우가 있었다.
구분자는 1BYTE이어야 하는 조건이 있었고 이를 만족하는 문자열은 데이터의 텍스트 칼럼에 이미 존재하였다. 여기서 parse를 제대로 적용해줘야 한다.
