# 6주차 강의 메모

상대적으로 딥러닝이 머신러닝보다는 feature engineering이 적다.

출력층: softmax function
-> 시그마(z)1이어야 함(i가 아닌)

binary는 0 또는 1

모든 가중치에 대해 편미분값을 계산하는 SGD로 학습하여 backpropation을 적용함. 
보통의 딥러닝 학습 모델에서 적용된다.

layer가 깊어질수록 High-order interaction이 깊어진다( 비선형성 추가 )

Wide and Deep : 딥러닝 모델 + 선형모델

DeepFM = DNN + FM
1-order + 2-order(내적곱) + high_order(concat)
각 field를 임베딩하는데 field간 내적곱을 해야하기 때문에 같은 차원이어야 한다.
(field는 성별, 연령 등에 대한 정보)
임베딩 차원을 정하려면 가장 큰 차원을 가진 field의 차원에 맞춰서 설정한다. 
임베딩 차원을 실험적으로 정하기도 하는데 차원이 크면 학습에 시간이 오래 걸린다. 

Deep Component
각 field를 임베딩하여 concat

116400 = 380*200 + 200 + 200*200 + 200
 
CTR 모델은 epoch를 한번만 돌려도 fit ( 많이 돌릴수록 오버피팅 발생할 수 있음 )
train_X에서 하나의 row는 각 필드를 조합한 i번째 열들이 됨

candidate generation -> multiclass classfication문제가 됨
따라서 softmax 가 마지막에 적용됨

사용자의 시청이력으로 pooling해야 함( onehot이 아닌 multihot 벡터로 압축 변환 )
얼마나 시청했는지( Weighted Logistic Regression ) 가중치로 두어 모델을 학습
유저 feature는 그대로 두고 아이템 feature(video) socre를 구해 ranking

유저 feature와 아이템 feature를 기반으로 랭킹
CF와 유사: 기존까지의 아이템 feauture와 연관성이 높은 비디오를 추천함

train loss : negative sample loss 도 포함되지만 validation loss는 전체 데이터에 대한 loss를 포함