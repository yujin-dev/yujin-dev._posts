---
title: "Airflow documentation - Tutorial"
category: "mlops"
---

airflow는 자동으로 스케줄링하고 모니터링하기 위한 플랫폼이다.
워크플로우를 DAG구조( Directed Acyclic Graphs )로 파악할 수 있다.

[ 특징 ]
- dynamic : airflow 파이프라인은 파이썬 코드로 작성한 configuration이다. 동적으로 코드를 수정 가능하다.
- extensible : operator를 정의하기 쉽다.
- elegant : Jinja를 활용한 scripts로 명확한 airflow 파이프라인을 만들 수 있다.
- scalable :  module과 message queue를 이용하여 임의의 workers를 운영할 수 있다.

[ 구조 ]  
![](https://res.cloudinary.com/bucketplace-co-kr/image/upload/w_1000/airflow_4.png)
- Scheduler : 모든 DAG와 Task에 대하여 모니터링 및 관리하고, 실행해야할 Task를 스케줄링
- Web server : Airflow의 웹 UI 서버
- DAG : Directed Acyclic Graph로 개발자가 Python으로 작성한 워크플로우
- Database : Airflow에 존재하는 DAG와 Task들의 메타데이터를 저장하는 데이터베이스
- Worker : 실제 Task를 실행하는 주체

## 설치
```console
$ pip install apache-airflow
```
airflow는 라이브러리이자 어플리케이션이다. contraint로 설치해야 작동할 수도 있다.
```console
$ pip install "apache-airflow[celery]==2.2.0" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.2.0/constraints-3.6.txt"
```

도커로도 설치 가능하다(https://airflow.apache.org/docs/docker-stack/index.html)

## 예시

### DAG
airflow 파이썬 스크립트는 단지 DAG 구조를 코드로 작성한 **설정** 파일이다. 
각각의 task는 각각 다른 worker에서 실행될 수 있다. 



```python
from datetime import datetime, timedelta
from textwrap import dedent
from airflow import DAG
from airflow.operators.bash import BashOperator


default_args = {
    'owner': 'airflow', 
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'pool': 'backfill',
    # 'priority_weight': 10,
    # 'end_date': datetime(2016, 1, 1),
    # 'wait_for_downstream': False,
    # 'dag': dag,
    # 'sla': timedelta(hours=2),
    # 'execution_timeout': timedelta(seconds=300),
    # 'on_failure_callback': some_function,
    # 'on_success_callback': some_other_function,
    # 'on_retry_callback': another_function,
    # 'sla_miss_callback': yet_another_function,
    # 'trigger_rule': 'all_success'
}
with DAG(
    'tutorial', # DAG를 구분할 고유의 dag_id
    default_args=default_args,
    description='A simple tutorial DAG',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=['example'],
) as dag:
    
    # task : operator 객체를 통해 task가 생성된다. 
    # task는 task_id, ownrer를 상속받아야 작동된다.
    t1 = BashOperator(
        task_id='print_date',
        bash_command='date',
    )

    t2 = BashOperator(
        task_id='sleep',
        depends_on_past=False,
        bash_command='sleep 5',
        retries=3, # default_args의 retries를 override
    )


    t1.doc_md = dedent(
        """\
    #### Task Documentation
    You can document your task using the attributes `doc_md` (markdown),
    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
    rendered in the UI's Task Instance Details page.
    ![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)

    """
    )

    # documentation 추가
    dag.doc_md = __doc__  
    dag.doc_md = """
    This is a documentation placed anywhere
    """
    # jinjga 템플릿을 통해 {}를 이용하여 입력할 수 있다.

    templated_command = dedent(
        """
    {% for i in range(5) %}
        echo "{{ ds }}"
        echo "{{ macros.ds_add(ds, 7)}}"
        echo "{{ params.my_param }}"
    {% endfor %}
    """
    )

    t3 = BashOperator(
        task_id='templated',
        depends_on_past=False,
        bash_command=templated_command,
        params={'my_param': 'Parameter I passed in'},
    )

    # dependency 설정
    t1 >> [t2, t3]
```

[ Documentation ]  
![](https://airflow.apache.org/docs/apache-airflow/stable/_images/task_doc.png)

[ DAG ]  
![](https://airflow.apache.org/docs/apache-airflow/stable/_images/dag_doc.png)

#### 실행

`airflow.cfg`에 설정한 DAGs 폴더에 위 코드가 기입된 `tutorial.py`가 있다고 할 때, 
```console
$ python ~/airflow/dags/tutorial.py
```
와 같이 실행할 수 있다.


[ command list ]  
```console
# initialize the database tables
$ airflow db init

# print the list of active DAGs
$ airflow dags list

# prints the list of tasks in the "tutorial" DAG
$ airflow tasks list tutorial

# prints the hierarchy of tasks in the "tutorial" DAG
$ airflow tasks list tutorial --tree
```

#### 테스트
특정 날짜에 task를 실행한다고 하자.
```console
$ airflow tasks test tutorial print_date 2021-10-26
```

### Pipeline
1. URL에서 받은 데이터를 `employees.csv`로 저장한다. 
2. `Employees`테이블에 삽입 전에 임시 테이블인 `Employees_temp`에 덤프한다.


```python
@dag(
    schedule_interval="0 0 * * *",
    start_date=datetime.today() - timedelta(days=2),
    dagrun_timeout=timedelta(minutes=60),
)
def Etl():
    @task
    def get_data():
        url = "https://raw.githubusercontent.com/apache/airflow/main/docs/apache-airflow/pipeline_example.csv"

        response = requests.request("GET", url)

        with open("/usr/local/airflow/dags/files/employees.csv", "w") as file:
            for row in response.text.split("\n"):
                file.write(row)

        postgres_hook = PostgresHook(postgres_conn_id="LOCAL")
        conn = postgres_hook.get_conn()
        cur = conn.cursor()
        with open("/usr/local/airflow/dags/files/employees.csv", "r") as file:
            cur.copy_from(
                f,
                "Employees_temp",
                columns=[
                    "Serial Number",
                    "Company Name",
                    "Employee Markme",
                    "Description",
                    "Leave",
                ],
                sep=",",
            )
        conn.commit()

    @task
    def merge_data():
        query = """
                delete
                from "Employees" e using "Employees_temp" et
                where e."Serial Number" = et."Serial Number";

                insert into "Employees"
                select *
                from "Employees_temp";
                """
        try:
            postgres_hook = PostgresHook(postgres_conn_id="LOCAL")
            conn = postgres_hook.get_conn()
            cur = conn.cursor()
            cur.execute(query)
            conn.commit()
            return 0
        except Exception as e:
            return 1

    get_data() >> merge_data()


dag = Etl()
```

배치에 적용하기 위해 해당 파이썬 파일을 `airflow/dags` 폴더에 추가한다. 이후 메인 폴더로 돌아가,
```console
$ docker-compose up airflow-init
```
를 실행한다.


## Concepts
### Executor
task가 실행되는 주체이다. 

[ Local Executors ]  
Sequential Executors 가 default이나, Local Executors로 변경하길 권장된다. single-machine에서 주로 사용된다.
- Debug Executors
- Local Executors : 병렬 지원
- Sequential Executors : 다중 연결이 지원되지 않는 `sqlite`를 사용할 때 적용

[ Remote Executors ]  
multi-machine이나 클라우드 환경에서 사용된다. 
- Celery Executor
- CeleryKubernetes Executor
- Dask Executor
- Kubernetes Executor

#### Local Executor
- Unlimited Parallelism : `LocalExecutor`에 task가 접수될 때마다 하나의 프로세스가 실행된다. task가 실행되고 `result_queue`에 결과가 저장되면 프로세스가 종료된다. 해당 프로세스는 `LocalWorker` class이다.  
- Limited Parallelism : `LocalExecutor`가 `self.parallelism`만큼 프로세스 수를 실행한다. 여기서 `task_queue`를 사용하여 해당 프로세스는 `QueuedLocalWorker` class이다.  

출처
- https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html
- https://www.bucketplace.co.kr/post/2021-04-13-%EB%B2%84%ED%82%B7%ED%94%8C%EB%A0%88%EC%9D%B4%EC%8A%A4-airflow-%EB%8F%84%EC%9E%85%EA%B8%B0/